---
title: "580Project"
author: "Zian Shang"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r}
# include packages
library(tidyverse)
library(caret)
library(corrplot)
library(randomForest)
library(nnet)
library(rpart)
library(rpart.plot)
library(xgboost)
```


```{r}
# read train.data
train.data <- read.csv("train.csv", 
                       header = T, 
                       na.strings = " ?") 

# check for NA
colSums(is.na(train.data))

###### should we impute instead of remove NA?

# remove NA
train.data <- na.omit(train.data)

# remove empty space in front of each char cell
# factorize char columns

# ***if needed, encoding char cells instead of factorizing them***

train.data[] <- lapply(train.data, 
                       function(x) {
                         if(is.character(x)) 
                           as.factor(trimws(x))
                         else x
                       })
view(train.data)
str(train.data)
```


### variable selection, identify important features

correlation analysis-> Tasfia
``` {r}
# Subset only numeric predictors
numeric_vars <- names(train.data)[sapply(train.data, is.numeric)]
numeric_vars
```

numeric_data <- train.data[, numeric_vars]

``` {r}
Compute correlation matrix
cor_matrix <- cor(numeric_data)
cor_matrix
```

``` {r}
# Display heat map
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8)
```

Feature importance scores using random forest:
``` {r}
# Load library
library(randomForest)

# Make sure the target is a factor
train.data$income <- as.factor(train.data$income)

# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(income ~ ., data = train.data, importance = TRUE, ntree = 500)

# Extract importance scores
importance_df <- as.data.frame(importance(rf_model))

# Add variable names as a column
importance_df$Variable <- rownames(importance_df)

# Reorder columns for clarity
importance_df <- importance_df[, c("Variable", "MeanDecreaseAccuracy", "MeanDecreaseGini")]

# Sort by MeanDecreaseGini (most commonly used)
importance_df <- importance_df[order(-importance_df$MeanDecreaseGini), ]

# Print the result
print(importance_df)
```
The Random Forest model identified relationship, marital.status, and capital.gain to play a critical role in the model's ability to differentiate between high and low income classes.

dimensionality reduction techniques -> Zian

outliers -> Matthew


###
tree-based models -> use education(categorical, factorized)
```{r}
train.data.1 <- subset(train.data, select = -c(education.num))
```

###
logistic, linear, NN models -> use education.num(numeric, original)
```{r}
train.data.2 <- subset(train.data, select = -c(education))
```

*** remember to optimize model parameters***

### model 1: multiple linear model -> Thomas
data scaling

### model 2: logistic model -> Zian
data scaling

### model 3: NN model -> Tasfia
data scaling

### model 4: CART model -> Nicole
do not scale

### model 5: random forest model -> Matthew
do not scale

### model 6: extra models, PCA, KNN... -> decide latter

### explore XgBoost classification --> Thomas





```{r}
# check for data imbalance
table(df$income)
prop.table(table(df$income))
# somewhat imbalanced
```

### ***if model predicts income>50K poorly, try handle imbalance***












```{r}
# read test data
test.data <-read.csv("test.csv", 
                     header = T, 
                     na.strings = " ?")

# remove NA
test.data <- na.omit(test.data)

# remove empty space in front of each char cell
# ***if needed, factorize/encode char cells***
test.data[] <- lapply(test.data, 
                       function(x) {
                         if(is.character(x)) trimws(x) else x
                       })



View(test.data) 
```


#### this line is used to test push to remove self branch & merge data to remove main branch ####























